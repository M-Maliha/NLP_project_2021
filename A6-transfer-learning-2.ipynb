{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this assignment, you will use a pre-trained convnet to produce features for a classifier that can detect a single object type. This notebook has some code to help you get started. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import os.path as osp\n",
    "from tqdm import tqdm_notebook as tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gather positive examples\n",
    "\n",
    "Pick a word. For example, \"red\" or \"santa\" or \"horse\". \n",
    "\n",
    "Now you will need to find \"positive\" image examples of that word. For example, if you chose \"red\" as your word, you will need to find images of red things. You are free to use Google Image search or something similar. File types shouldn't matter, but try to stick with .png and .jpg files.\n",
    "\n",
    "You'll need at least 100 positive example images. Put them in the folder called `pos`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gather negative examples\n",
    "\n",
    "Now you need to think about negative examples; i.e., things that are *not* examples of your word. You can either just find random images, or look for specific negative examples. For example, if you chose the word \"red\" then it might work best if you find negative examples that are other colors, especially colors close to red. \n",
    "\n",
    "You'll need at least 200 negative example images. Put them in the folder called `neg`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.) Run the following cell\n",
    "\n",
    "* This imports needed Keras libraries\n",
    "* Then, it gets the trained VGG19 imagenet model\n",
    "* Then, it prints out the names of all the layers in that model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_4\n",
      "block1_conv1\n",
      "block1_conv2\n",
      "block1_pool\n",
      "block2_conv1\n",
      "block2_conv2\n",
      "block2_pool\n",
      "block3_conv1\n",
      "block3_conv2\n",
      "block3_conv3\n",
      "block3_conv4\n",
      "block3_pool\n",
      "block4_conv1\n",
      "block4_conv2\n",
      "block4_conv3\n",
      "block4_conv4\n",
      "block4_pool\n",
      "block5_conv1\n",
      "block5_conv2\n",
      "block5_conv3\n",
      "block5_conv4\n",
      "block5_pool\n",
      "flatten\n",
      "fc1\n",
      "fc2\n",
      "predictions\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.applications import VGG19\n",
    "from tensorflow.keras.applications.vgg19 import preprocess_input\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "base_model = VGG19(weights='imagenet',include_top=True)\n",
    "xs,ys=224,224\n",
    "\n",
    "for layer in base_model.layers:\n",
    "    print(layer.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.engine.functional.Functional at 0x7fcf22ea1430>"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.) Determine your output layer\n",
    "\n",
    "- try `predictions` first\n",
    "- note the layers printed out above; you can use any of those laters\n",
    "- pay attention to output shape of each layer! predictions is a vector of size 1000, for example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer = 'predictions'\n",
    "\n",
    "model = Model(inputs=base_model.input, outputs=base_model.get_layer(layer).output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the following cell\n",
    "\n",
    "- These functions are to help you perform transfer learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image(img_path, xs,ys):\n",
    "    x = image.load_img(img_path, target_size=(xs, ys))\n",
    "    x = image.img_to_array(x)\n",
    "    x = np.expand_dims(x, axis=0)\n",
    "    return x\n",
    "\n",
    "def get_img_features(model, img):\n",
    "    img = preprocess_input(img)\n",
    "    yhat = model.predict(img)\n",
    "    return yhat\n",
    "\n",
    "def get_image_features(word):\n",
    "    files = [f for f in listdir(word)] # grab all of the images in the folder\n",
    "    image_vectors = []\n",
    "    for f in tqdm(files):\n",
    "        img = get_image(osp.join(word, f), xs, ys) \n",
    "        x_feats = get_img_features(model, img).flatten() # get features for each image\n",
    "        image_vectors.append(x_feats) \n",
    "    return np.array(image_vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.) Evaluate a classifier for your `word`\n",
    "\n",
    "* Using the positive and negative output from `model`, train a classifier (it can be a linear classifier from scikit-learn, if you'd like, but I would recommend the Keras Dense network we built for the previous assignment). \n",
    "* You'll need to split your data into Train and Test (I would recommend using half of the data for training, half for testing; you may opt for downloading more positive and negative examples)\n",
    "* your classifier can be any scikit classifier, but you can also use a neural network of some kind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-7-729984638232>:15: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  for f in tqdm(files):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c00503139074ffcb3a4fdede92c647f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pos_images = get_image_features('pos') # get positive image vectors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-91-729984638232>:15: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  for f in tqdm(files):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "293709bddc74414d838a35a3b34c5627",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/198 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "neg_images = get_image_features('neg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare the data. Split to train/test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>991</th>\n",
       "      <th>992</th>\n",
       "      <th>993</th>\n",
       "      <th>994</th>\n",
       "      <th>995</th>\n",
       "      <th>996</th>\n",
       "      <th>997</th>\n",
       "      <th>998</th>\n",
       "      <th>999</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3.507573e-07</td>\n",
       "      <td>2.879765e-06</td>\n",
       "      <td>1.580464e-07</td>\n",
       "      <td>2.802052e-07</td>\n",
       "      <td>2.935750e-07</td>\n",
       "      <td>4.441483e-05</td>\n",
       "      <td>2.204816e-07</td>\n",
       "      <td>2.279076e-05</td>\n",
       "      <td>3.832250e-05</td>\n",
       "      <td>6.307426e-07</td>\n",
       "      <td>...</td>\n",
       "      <td>5.019204e-05</td>\n",
       "      <td>2.662022e-04</td>\n",
       "      <td>5.218061e-04</td>\n",
       "      <td>1.818533e-03</td>\n",
       "      <td>5.209767e-05</td>\n",
       "      <td>0.000341</td>\n",
       "      <td>2.576908e-04</td>\n",
       "      <td>0.002303</td>\n",
       "      <td>1.040072e-05</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.175289e-07</td>\n",
       "      <td>4.842361e-05</td>\n",
       "      <td>2.520233e-06</td>\n",
       "      <td>3.377019e-07</td>\n",
       "      <td>2.134688e-06</td>\n",
       "      <td>1.307097e-06</td>\n",
       "      <td>4.497871e-07</td>\n",
       "      <td>5.252026e-06</td>\n",
       "      <td>4.435612e-06</td>\n",
       "      <td>2.679820e-08</td>\n",
       "      <td>...</td>\n",
       "      <td>9.435138e-05</td>\n",
       "      <td>1.044118e-05</td>\n",
       "      <td>2.925503e-05</td>\n",
       "      <td>1.641060e-05</td>\n",
       "      <td>8.903983e-07</td>\n",
       "      <td>0.001081</td>\n",
       "      <td>1.784793e-04</td>\n",
       "      <td>0.000147</td>\n",
       "      <td>1.295121e-06</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5.573030e-09</td>\n",
       "      <td>3.964879e-07</td>\n",
       "      <td>1.310729e-09</td>\n",
       "      <td>2.594938e-10</td>\n",
       "      <td>5.760505e-10</td>\n",
       "      <td>8.320185e-09</td>\n",
       "      <td>4.934670e-10</td>\n",
       "      <td>7.858988e-10</td>\n",
       "      <td>3.724695e-09</td>\n",
       "      <td>8.338656e-10</td>\n",
       "      <td>...</td>\n",
       "      <td>1.552750e-08</td>\n",
       "      <td>1.016937e-06</td>\n",
       "      <td>2.049984e-09</td>\n",
       "      <td>1.034703e-08</td>\n",
       "      <td>9.547201e-09</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>1.804308e-07</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>4.665858e-08</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6.226661e-09</td>\n",
       "      <td>2.628523e-07</td>\n",
       "      <td>6.355290e-08</td>\n",
       "      <td>1.300558e-07</td>\n",
       "      <td>7.130999e-08</td>\n",
       "      <td>3.917478e-07</td>\n",
       "      <td>2.130303e-08</td>\n",
       "      <td>1.038874e-08</td>\n",
       "      <td>1.411610e-08</td>\n",
       "      <td>5.910153e-10</td>\n",
       "      <td>...</td>\n",
       "      <td>1.463722e-07</td>\n",
       "      <td>2.973103e-07</td>\n",
       "      <td>1.120569e-08</td>\n",
       "      <td>2.879135e-08</td>\n",
       "      <td>2.308130e-08</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>2.330296e-06</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>7.411234e-08</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.758125e-07</td>\n",
       "      <td>2.110604e-05</td>\n",
       "      <td>2.189291e-07</td>\n",
       "      <td>1.920241e-08</td>\n",
       "      <td>3.774171e-08</td>\n",
       "      <td>9.923679e-07</td>\n",
       "      <td>8.733600e-08</td>\n",
       "      <td>2.401701e-06</td>\n",
       "      <td>3.107620e-06</td>\n",
       "      <td>1.342046e-07</td>\n",
       "      <td>...</td>\n",
       "      <td>5.257853e-06</td>\n",
       "      <td>1.477315e-03</td>\n",
       "      <td>7.170216e-06</td>\n",
       "      <td>1.740215e-05</td>\n",
       "      <td>1.772866e-05</td>\n",
       "      <td>0.000060</td>\n",
       "      <td>5.179733e-03</td>\n",
       "      <td>0.000084</td>\n",
       "      <td>6.189363e-07</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 1001 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              0             1             2             3             4  \\\n",
       "0  3.507573e-07  2.879765e-06  1.580464e-07  2.802052e-07  2.935750e-07   \n",
       "1  1.175289e-07  4.842361e-05  2.520233e-06  3.377019e-07  2.134688e-06   \n",
       "2  5.573030e-09  3.964879e-07  1.310729e-09  2.594938e-10  5.760505e-10   \n",
       "3  6.226661e-09  2.628523e-07  6.355290e-08  1.300558e-07  7.130999e-08   \n",
       "4  1.758125e-07  2.110604e-05  2.189291e-07  1.920241e-08  3.774171e-08   \n",
       "\n",
       "              5             6             7             8             9  ...  \\\n",
       "0  4.441483e-05  2.204816e-07  2.279076e-05  3.832250e-05  6.307426e-07  ...   \n",
       "1  1.307097e-06  4.497871e-07  5.252026e-06  4.435612e-06  2.679820e-08  ...   \n",
       "2  8.320185e-09  4.934670e-10  7.858988e-10  3.724695e-09  8.338656e-10  ...   \n",
       "3  3.917478e-07  2.130303e-08  1.038874e-08  1.411610e-08  5.910153e-10  ...   \n",
       "4  9.923679e-07  8.733600e-08  2.401701e-06  3.107620e-06  1.342046e-07  ...   \n",
       "\n",
       "            991           992           993           994           995  \\\n",
       "0  5.019204e-05  2.662022e-04  5.218061e-04  1.818533e-03  5.209767e-05   \n",
       "1  9.435138e-05  1.044118e-05  2.925503e-05  1.641060e-05  8.903983e-07   \n",
       "2  1.552750e-08  1.016937e-06  2.049984e-09  1.034703e-08  9.547201e-09   \n",
       "3  1.463722e-07  2.973103e-07  1.120569e-08  2.879135e-08  2.308130e-08   \n",
       "4  5.257853e-06  1.477315e-03  7.170216e-06  1.740215e-05  1.772866e-05   \n",
       "\n",
       "        996           997       998           999    y  \n",
       "0  0.000341  2.576908e-04  0.002303  1.040072e-05  1.0  \n",
       "1  0.001081  1.784793e-04  0.000147  1.295121e-06  1.0  \n",
       "2  0.000002  1.804308e-07  0.000001  4.665858e-08  1.0  \n",
       "3  0.000006  2.330296e-06  0.000004  7.411234e-08  1.0  \n",
       "4  0.000060  5.179733e-03  0.000084  6.189363e-07  1.0  \n",
       "\n",
       "[5 rows x 1001 columns]"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = np.concatenate((pos_images, neg_images))\n",
    "\n",
    "y_pos = np.ones(pos_images.shape[0])\n",
    "y_neg = np.zeros(neg_images.shape[0])\n",
    "y = np.concatenate((y_pos, y_neg))\n",
    "\n",
    "data = pd.DataFrame(X)\n",
    "data['y'] = y\n",
    "\n",
    "data.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Splitting "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.5, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((149, 1000), (149, 1000))"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define model, train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1, max_iter=1000, random_state=0, verbose=1)"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier = LogisticRegression(random_state=0, C=1, max_iter=1000, verbose=1)\n",
    "classifier.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 85.235\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-99-c888f9d830cc>:3: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  accuracy = np.mean((y_test == predictions).astype(np.float)) * 100.\n"
     ]
    }
   ],
   "source": [
    "# Evaluate using the logistic regression classifier\n",
    "predictions = classifier.predict(X_test)\n",
    "accuracy = np.mean((y_test == predictions).astype(np.float)) * 100.\n",
    "print(f\"Accuracy = {accuracy:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.) Try CLIP\n",
    "\n",
    "* Repeat steps 3 and 4 above, only this time using the [CLIP](https://github.com/openai/CLIP) model\n",
    "  \n",
    "  To get image features, use the following example: `image = preprocess(Image.open(\"CLIP.png\")).unsqueeze(0).to(device)`\n",
    "\n",
    "(see also the last code section of the README for the CLIP github repo on training a classifier using CLIP features)\n",
    "  \n",
    "  \n",
    "* (Answer in a markdown cell): Which model+layer works the best for this data? Why do you think that is?\n",
    "* What makes for good positive examples? What makes for good negative examples? Why does the choice of negative examples matter?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import clip\n",
    "import torch\n",
    "from PIL import Image\n",
    "import transformers \n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from torch.utils.data import DataLoader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['RN50',\n",
       " 'RN101',\n",
       " 'RN50x4',\n",
       " 'RN50x16',\n",
       " 'RN50x64',\n",
       " 'ViT-B/32',\n",
       " 'ViT-B/16',\n",
       " 'ViT-L/14']"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clip.available_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import clip\n",
    "import torch\n",
    "from torchvision.datasets import CIFAR100\n",
    "\n",
    "# Load the model\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model, preprocess = clip.load('ViT-B/32', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/maishamaliha/Documents/NLP/A6'"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "direc = '/Users/maishamaliha/Documents/NLP/A6' + '/sample'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import PIL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = glob.glob(direc+'/*/*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label probs: [[0.3859106 0.6140894]]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import clip\n",
    "from PIL import Image\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model, preprocess = clip.load(\"ViT-B/32\", device=device)\n",
    "\n",
    "image = preprocess(Image.open(images[0])).unsqueeze(0).to(device)\n",
    "text = clip.tokenize([\"pizza\", \"not pizza\"]).to(device)\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    image_features = model.encode_image(image)\n",
    "    text_features = model.encode_text(text)\n",
    "    \n",
    "    logits_per_image, logits_per_text = model(image, text)\n",
    "    probs = logits_per_image.softmax(dim=-1).cpu().numpy()\n",
    "\n",
    "print(\"Label probs:\", probs)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/maishamaliha/Documents/NLP/A6/sample/neg copy/photo-1565138146061-e29b079736c0.jpg'"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
